{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/katsuyut/Deep_project/blob/master/starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "J0oHCQWtlvtQ",
    "outputId": "3e2e625f-6b13-4c80-94a5-13db95ba21b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Deep_project'...\n",
      "remote: Enumerating objects: 41326, done.\u001b[K\n",
      "remote: Total 41326 (delta 0), reused 0 (delta 0), pack-reused 41326\u001b[K\n",
      "Receiving objects: 100% (41326/41326), 796.06 MiB | 40.56 MiB/s, done.\n",
      "Resolving deltas: 100% (20277/20277), done.\n",
      "Checking out files: 100% (41166/41166), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/katsuyut/Deep_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9VlujeVlmeAT",
    "outputId": "70b9a46b-36bc-4c60-cef9-e65d5109b303"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Deep_project/code\n"
     ]
    }
   ],
   "source": [
    "%cd Deep_project/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jKHaNgeRpBkq",
    "outputId": "3c46c3a4-2c78-4691-d5ff-ea37d1659f03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all-dogs']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "t1Fs6u0zJV7T",
    "outputId": "d84e74a4-5a53-4949-e55b-ed735fb79e8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "p1yViUsiJkVG",
    "outputId": "4bdd1a84-814b-4869-d18d-6e2c8cffd2a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gputil\n",
      "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
      "Building wheels for collected packages: gputil\n",
      "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7410 sha256=ada79d648e2c2dcecb2a4c6c2c8e9c16e2f4cda12488249b071c8834b4f83358\n",
      "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
      "Successfully built gputil\n",
      "Installing collected packages: gputil\n",
      "Successfully installed gputil-1.4.0\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
      "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
      "Gen RAM Free: 12.7 GB  | Proc size: 261.0 MB\n",
      "GPU RAM Free: 11430MB | Used: 11MB | Util   0% | Total 11441MB\n"
     ]
    }
   ],
   "source": [
    "# memory footprint support libraries/code\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
    "gpu = GPUs[0]\n",
    "def printm():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "printm() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "7Y4wwtLXpH85",
    "outputId": "1431b9f4-7cd9-4d86-c839-c3c14f64f2cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaa 6.125643491744995\n",
      "[1/2][0/644] Loss_D: 1.3179 Loss_G: 3.3759 D(x): 0.4690 D(G(z)): 0.4264 / 0.0240\n",
      "aaaaaa 6.384887456893921\n",
      "aaaaaa 6.664268255233765\n",
      "aaaaaa 6.925837755203247\n",
      "aaaaaa 7.187266111373901\n",
      "aaaaaa 7.443449020385742\n",
      "aaaaaa 7.701140642166138\n",
      "aaaaaa 7.968158006668091\n",
      "aaaaaa 8.226749181747437\n",
      "aaaaaa 8.459534168243408\n",
      "aaaaaa 8.713546991348267\n",
      "aaaaaa 8.955114603042603\n",
      "aaaaaa 9.19005823135376\n",
      "aaaaaa 9.45176362991333\n",
      "aaaaaa 9.693874835968018\n",
      "aaaaaa 9.960147142410278\n",
      "aaaaaa 10.241369724273682\n",
      "aaaaaa 10.486260890960693\n",
      "aaaaaa 10.737467765808105\n",
      "aaaaaa 10.981387615203857\n",
      "aaaaaa 11.222510814666748\n",
      "aaaaaa 11.474661588668823\n",
      "aaaaaa 11.730021476745605\n",
      "aaaaaa 11.982871532440186\n",
      "aaaaaa 12.22628402709961\n",
      "aaaaaa 12.470815420150757\n",
      "aaaaaa 12.712669610977173\n",
      "aaaaaa 12.986629009246826\n",
      "aaaaaa 13.229555606842041\n",
      "aaaaaa 13.491215944290161\n",
      "aaaaaa 13.748623847961426\n",
      "aaaaaa 14.014463901519775\n",
      "aaaaaa 14.237706899642944\n",
      "aaaaaa 14.464367151260376\n",
      "aaaaaa 14.723462104797363\n",
      "aaaaaa 14.960159301757812\n",
      "aaaaaa 15.206342220306396\n",
      "aaaaaa 15.43838095664978\n",
      "aaaaaa 15.678284883499146\n",
      "aaaaaa 15.916233539581299\n",
      "aaaaaa 16.173044681549072\n",
      "aaaaaa 16.40621042251587\n",
      "aaaaaa 16.662948608398438\n",
      "aaaaaa 16.908262014389038\n",
      "aaaaaa 17.144896984100342\n",
      "aaaaaa 17.38280987739563\n",
      "aaaaaa 17.63698434829712\n",
      "aaaaaa 17.90491533279419\n",
      "aaaaaa 18.150712728500366\n",
      "aaaaaa 18.440096855163574\n",
      "aaaaaa 18.68791174888611\n",
      "aaaaaa 18.933378219604492\n",
      "aaaaaa 19.1791729927063\n",
      "aaaaaa 19.432881593704224\n",
      "aaaaaa 19.696898698806763\n",
      "aaaaaa 19.990676403045654\n",
      "aaaaaa 20.27941370010376\n",
      "aaaaaa 20.51979088783264\n",
      "aaaaaa 20.802685499191284\n",
      "aaaaaa 21.038387775421143\n",
      "aaaaaa 21.296032428741455\n",
      "aaaaaa 21.532572507858276\n",
      "aaaaaa 21.76946449279785\n",
      "aaaaaa 22.015148639678955\n",
      "aaaaaa 22.25902223587036\n",
      "aaaaaa 22.50275421142578\n",
      "aaaaaa 22.751510620117188\n",
      "aaaaaa 22.981108903884888\n",
      "aaaaaa 23.23993730545044\n",
      "aaaaaa 23.48608946800232\n",
      "aaaaaa 23.724121809005737\n",
      "aaaaaa 23.972731113433838\n",
      "aaaaaa 24.221948385238647\n",
      "aaaaaa 24.54921317100525\n",
      "aaaaaa 24.806742191314697\n",
      "aaaaaa 25.062732696533203\n",
      "aaaaaa 25.310237407684326\n",
      "aaaaaa 25.567919969558716\n",
      "aaaaaa 25.818975687026978\n",
      "aaaaaa 26.08525562286377\n",
      "aaaaaa 26.332191467285156\n",
      "aaaaaa 26.56664514541626\n",
      "aaaaaa 26.82337522506714\n",
      "aaaaaa 27.061211585998535\n",
      "aaaaaa 27.314730405807495\n",
      "aaaaaa 27.560824155807495\n",
      "aaaaaa 27.81583547592163\n",
      "aaaaaa 28.093249320983887\n",
      "aaaaaa 28.384523391723633\n",
      "aaaaaa 28.613618850708008\n",
      "aaaaaa 28.858840703964233\n",
      "aaaaaa 29.10781240463257\n",
      "aaaaaa 29.35611081123352\n",
      "aaaaaa 29.58984923362732\n",
      "aaaaaa 29.843220472335815\n",
      "aaaaaa 30.10779070854187\n",
      "aaaaaa 30.34520411491394\n",
      "aaaaaa 30.601979970932007\n",
      "aaaaaa 30.855542421340942\n",
      "aaaaaa 31.118112325668335\n",
      "aaaaaa 31.369076251983643\n",
      "aaaaaa 31.606701135635376\n",
      "aaaaaa 31.888306617736816\n",
      "aaaaaa 32.13434934616089\n",
      "aaaaaa 32.3748197555542\n",
      "aaaaaa 32.62180542945862\n",
      "aaaaaa 32.86967992782593\n",
      "aaaaaa 33.1236732006073\n",
      "aaaaaa 33.36422038078308\n",
      "aaaaaa 33.60053825378418\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a0397cf6c1e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0merrG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0merrG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mD_G_z2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0moptimizerG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'aaaaaa'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "\n",
    "######\n",
    "import time\n",
    "start = time.time()\n",
    "######\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# 64x64 images!\n",
    "transform = transforms.Compose([transforms.Resize(64),\n",
    "                                transforms.CenterCrop(64),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_data = datasets.ImageFolder('../input/', transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, shuffle=True,\n",
    "                                           batch_size=batch_size)\n",
    "\n",
    "imgs, label = next(iter(train_loader))\n",
    "imgs = imgs.numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz, nfeats, nchannels):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # input is Z, going into a convolution\n",
    "        self.conv1 = nn.ConvTranspose2d(nz, nfeats * 8, 4, 1, 0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(nfeats * 8)\n",
    "        # state size. (nfeats*8) x 4 x 4\n",
    "        \n",
    "        self.conv2 = nn.ConvTranspose2d(nfeats * 8, nfeats * 8, 4, 2, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(nfeats * 8)\n",
    "        # state size. (nfeats*8) x 8 x 8\n",
    "        \n",
    "        self.conv3 = nn.ConvTranspose2d(nfeats * 8, nfeats * 4, 4, 2, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(nfeats * 4)\n",
    "        # state size. (nfeats*4) x 16 x 16\n",
    "        \n",
    "        self.conv4 = nn.ConvTranspose2d(nfeats * 4, nfeats * 2, 4, 2, 1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(nfeats * 2)\n",
    "        # state size. (nfeats * 2) x 32 x 32\n",
    "        \n",
    "        self.conv5 = nn.ConvTranspose2d(nfeats * 2, nfeats, 4, 2, 1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(nfeats)\n",
    "        # state size. (nfeats) x 64 x 64\n",
    "        \n",
    "        self.conv6 = nn.ConvTranspose2d(nfeats, nchannels, 3, 1, 1, bias=False)\n",
    "        # state size. (nchannels) x 64 x 64\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)))\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)))\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)))\n",
    "        x = F.leaky_relu(self.bn5(self.conv5(x)))\n",
    "        x = torch.tanh(self.conv6(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nchannels, nfeats):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # input is (nchannels) x 64 x 64\n",
    "        self.conv1 = nn.Conv2d(nchannels, nfeats, 4, 2, 1, bias=False)\n",
    "        # state size. (nfeats) x 32 x 32\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(nfeats, nfeats * 2, 4, 2, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(nfeats * 2)\n",
    "        # state size. (nfeats*2) x 16 x 16\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(nfeats * 2, nfeats * 4, 4, 2, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(nfeats * 4)\n",
    "        # state size. (nfeats*4) x 8 x 8\n",
    "       \n",
    "        self.conv4 = nn.Conv2d(nfeats * 4, nfeats * 8, 4, 2, 1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(nfeats * 8)\n",
    "        # state size. (nfeats*8) x 4 x 4\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(nfeats * 8, 1, 4, 1, 0, bias=False)\n",
    "        # state size. 1 x 1 x 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)), 0.2)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)), 0.2)\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)), 0.2)\n",
    "        x = torch.sigmoid(self.conv5(x))\n",
    "        \n",
    "        return x.view(-1, 1)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lr = 0.0003\n",
    "beta1 = 0.5\n",
    "\n",
    "netG = Generator(100, 32, 3).to(device)\n",
    "netD = Discriminator(3, 48).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "nz = 100\n",
    "fixed_noise = torch.randn(25, nz, 1, 1, device=device)\n",
    "\n",
    "real_label = 0.9\n",
    "fake_label = 0\n",
    "batch_size = train_loader.batch_size\n",
    "\n",
    "\n",
    "\n",
    "### training here\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "step = 0\n",
    "for epoch in range(epochs):\n",
    "    for ii, (real_images, train_labels) in enumerate(train_loader):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        real_images = real_images.to(device)\n",
    "        batch_size = real_images.size(0)\n",
    "        labels = torch.full((batch_size, 1), real_label, device=device)\n",
    "\n",
    "        output = netD(real_images)\n",
    "        errD_real = criterion(output, labels)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        labels.fill_(fake_label)\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, labels)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        labels.fill_(real_label)  # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, labels)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "#         print('aaaaaa', time.time() - start)\n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "                  % (epoch + 1, epochs, ii, len(train_loader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "            \n",
    "        step += 1\n",
    "        \n",
    "        \n",
    "# torch.save(netG.state_dict(), 'generator.pth')\n",
    "# torch.save(netD.state_dict(), 'discriminator.pth')\n",
    "\n",
    "\n",
    "if not os.path.exists('../output_images'):\n",
    "    os.mkdir('../output_images')\n",
    "im_batch_size = 50\n",
    "n_images=10000\n",
    "for i_batch in range(0, n_images, im_batch_size):\n",
    "    gen_z = torch.randn(im_batch_size, 100, 1, 1, device=device)\n",
    "    gen_images = netG(gen_z)\n",
    "    images = gen_images.to(\"cpu\").clone().detach()\n",
    "    images = images.numpy().transpose(0, 2, 3, 1)\n",
    "    for i_image in range(gen_images.size(0)):\n",
    "        save_image(gen_images[i_image, :, :, :], os.path.join('../output_images', f'image_{i_batch+i_image:05d}.png'))\n",
    "\n",
    "\n",
    "import shutil\n",
    "shutil.make_archive('images', 'zip', '../output_images')\n",
    "\n",
    "######\n",
    "elapsed_time = time.time() - start\n",
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "95hZc_QSpg6w"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "starter.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
